\subsection{循环图神经网络}
循环图神经网络(RecGNNs)是GNN的先驱作品。它们在图中的节点上循环应用相同的参数集，以提取高级节点表示。受计算能力的限制，早期的研究主要集中在有向无环图。

最早由Scarselli等人提出的图神经网络($GNN^*$)\cite{scarselli2008graph}，扩展了先前的递归模型以处理一般类型的图，例如，有环、无环、有向和无向图。基于信息扩散机制，$GNN^*$通过周期性地交换邻域信息来更新节点的状态，直到达到稳定的均衡。节点的隐藏状态通过以下方式反复更新。
\[
h_v^{(t)}=\sum_{u\in N(v)}f(x_v,x^e_{v,u},x_u,h_u^{t-1})
\]
其中$f$是参数的函数，$h_v^{0}$是随机初始化的。求和操作使$GNN^*$适用于所有节点，即使邻居的数量不同并且没有邻域排序。与此同时，为了确保收敛，循环函数必须是一个收缩映射，它会缩小映射后两点之间的距离。
在是神经网络的情况下，必须对$Jacobian$参数矩阵施加惩罚项。当满足收敛标准时，最后一步节点隐藏状态被转发到读取层。$GNN^*$交替节点状态传播的阶段和参数梯度计算的阶段以最小化训练目标。该策略使$GNN^*$能够处理有环图。

图形回声状态网络(GraphESN)\cite{gallicchio2010graph}扩展了回声状态网络以提高效率。GraphESN由编码器和输出层组成。另外，编码器随机初始化因此无需训练。它实现了一个收缩状态转换函数，以反复更新节点状态，直到全局图状态达到收敛。然后，通过将固定节点状态作为输入来训练输出层。

门控图神经网络(GGNN)\cite{li2015gated}采用门控循环单元(GRU)作为循环函数，将循环减少到固定的次数。其优点是不再需要约束参数来确保收敛。这可以极大的简化训练。

节点的隐藏状态通过其先前的隐藏状态以及相邻的隐藏状态进行更新，更新公式定义如下。
\[
h_v^{(t)}=GRU(h_u^{t-1},\sum_{u\in N(v)} Wh_u^{(t)})
\]
其中，$h_v^{(0)}=x_v$,与前两种方法的不同点在于，GGNN使用反向传播的算法来学习模型的参数。对于比较大的图来说，可能会有一些麻烦。因为GGNN需要在每个节点上使用多次循环函数，并将所有的中间状态都进行存储。所以GGNN不太适合于训练较大的网络。

随机稳态嵌入(SSE)提出了一种学习算法，该算法对大型图更具可稳定性。SSE以随机和异步的方式周期性地更新节点的隐藏状态，它对一批用于状态更新的节点和一批用于梯度计算的节点进行采样。为了保持稳定性，SSE的循环函数定义为历史状态和新状态的加权平均值，其形式为
\[
h_v^{(t)}=(1-\alpha)h_u^{t-1}+\alpha W_1\delta(W_2[x_v,\sum_{u\in N(v)}[h_u^{(t-1)},x_u]]) ,
\]
其中，$\alpha$是超参数，$h_v^{(0)}$是随机初始化的。


