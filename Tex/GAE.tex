\subsection{图自动编码器}
图自动编码器（GAE）是一种深度神经架构，它能够将节点映射到潜在特征空间并从潜在表示中解码图信息。GAE可以用来学习网络嵌入或者生成新的图。

\subsubsection{网络嵌入}
网络嵌入是节点的低维表示，其保留了节点的拓扑信息。GAE使用编码器学习网络嵌入以提取网络嵌入并使用解码器强制网络嵌入以通过重建正向点互信息矩阵（PPMI），邻接矩阵等来保存节点的拓扑信息，如同\ref{sec:nodeem}节谈到的节点表示学习一样。

较早的方法主要使用多层感知器来构建用于网络嵌入学习的GAE。用于图形表示的深度神经网络（DNGR）\cite{cao2016deep}使用堆叠的去噪自动编码器过多层感知器对逐点互信息矩阵（PPMI）进行编码和解码。所学习的潜在表示能够保留数据背后的高度非线性规则性，并且特别是在缺少值时具有很好的鲁棒性。PPMI矩阵通过从图表中采样的随机游走本质上捕获节点共现信息。PPMI矩阵定义为
\[PPMI_{v_1, v_2} = \max(\log (\frac{count(v_1, v_2)\cdot |D|}{count(v_1)count(v_2)}), 0)\]

与此同时， 结构深度网络嵌入（SDNE）\cite{wang2016structural}使用堆叠式自动编码器共同保存节点的一阶相似度度和二阶相似度，一阶相似度和二阶相似度的定义见\ref{sec:nodeem}节内容。SDNE在编码器的输出和解码器的输出上分别提出两个损耗函数。一阶损失函数使学习到的网络嵌入能够通过最小化节点的网络嵌入与其邻居的网络嵌入之间的距离来保持节点的一阶相似度。而二阶损失函数使学习到的网络嵌入能够通过最小化节点输入与其重构输入之间的距离来保留节点的二阶相似度。具体地，一阶损失函数和二阶损失函数的定义如下：
\begin{align*}
  &L_{1st} = \sum_{(v, u) \in E}A_{u, v} ||enc(X_v) - enc(X_v)||\\
  & L_{2nd} = \sum_{v\in V} ||dec(enc(V_v)) - X_v||\bigodot b_v||^2
\end{align*}

图自动编码器（GAE*）\cite{61}利用GCN同时编码节点结构信息和节点特征信息。GAE*的编码器由两个图卷积层组成，其形式为
\[Z = enc(X, A) = Gconv (f(Gconv(A, X; \theta_1)); \theta_2)\]
GAE*的解码器旨在从它们的节点关系信息中解码通过重建图邻接矩阵进行嵌入，其定义为：
\[A_{v, u} = dec(z_v, z_u) = \sigma(z_v^Tz_u)\]

由于自动编码器的容量，简单地重建图邻接矩阵可能会导致过度拟合。变异图自动编码器（VGAE）\cite{kipf2016variational}是GAE的变异版本，用于学习数据的分布，最小化目标函数为两个分布的$KL$散度。为了进一步强制后验分布近似于先验分布，对抗正则化变分图自动编码器（ARVGA）\cite{pan2018adversarially}采用了生成对抗网络（GAN）的训练方案。GAN在训练生成模型时在生成器和判别器之间进行对抗。生成器尝试生成尽可能真实的“伪样本”，而鉴别器试图将“伪样本”与真实样本区分开。

\subsubsection{图生成}

当有很多个图时，GAE可以通过将图编码为潜在表示，并解码给定潜在表示的图结构来学习图的生成分布。大多数用于图生成的GAE都是为解决分子图形生成问题而设计的，该问题在药物发现中具有很高的实用价值。这些方法基本都以顺序方式或全局方式生成新的图。

顺序方法通过逐步生成节点和边进而生成图。Gomez\cite{gomez2018automatic}，Kusner\cite{kusner2017grammar}和Dai\cite{dai2018syntax}用深CNN和RNN分别作为编码器和解码器，对名为SMILES的分子图的字符串表示的生成过程进行建模。尽管这些方法是特定于领域的，但也可以通过将节点和边迭代地添加到增长的图上，直到满足特定条件为止，从而适用于一般图。图的深度生成模型（DeepGMG）\cite{li2018learning}假设图的概率是所有可能的节点排列之和：
\[p(G) = \sum_{\pi}p(G, \pi)\]
其中$\pi$表示节点顺序，它捕获图中所有节点和边的复杂联合概率。DeepGMG通过一系列决策生成图形，即是否添加节点、添加哪个节点、是否添加边以及连接到新节点的节点。生成节点和边的决策过程取决于由RecGNN更新的增长图的节点状态和图状态。在另一项工作中，graph RNN\cite{you2018graphrnn}提出了一个图级RNN和一个边级RNN来模拟节点和边的生成过程。图级RNN每次都向节点序列添加一个新节点，而边级RNN生成一个二进制序列，指示新节点与序列中先前生成的节点之间的连接。

全局方法则一次性输出整个图。图变量自动编码器（GraphVAE）\cite{simonovsky2018graphvae}将节点和边的存在建模为独立随机变量。通过假设由编码器定义的后验分布$q_{\phi}(z|G)$和由解码器定义的生成分布$p_{\theta}(G|z)$，GraphVAE优化了变分下界
\[L(\phi, \theta;G) = E_{q_{\phi}}(z|G)[-\log p_{\theta}(G|z)] + KL[q_{\phi}(z|G)||p(z)]\]

使用ConvGNN作为编码器，并使用简单的多层感知层作为解码器，GraphVAE输出具有其邻接矩阵，节点属性和边属性的生成图形。控制生成图的全局属性（例如图连接性，有效性和节点兼容性）具有挑战性，正则图可变自动编码器（RGVAE）\cite{ma2018constrained}进一步在图变自动编码器上施加了有效性约束，以正则化解码器的输出分布。分子生成对抗网络（MolGAN）\cite{de2018molgan}整合了ConvGNNs、GANs和强化学习目标，以生成具有所需特性的图形。MolGAN由生成器和判别器组成，它们相互竞争以提高生成器的真实性。在MolGAN中，生成器尝试提出一个伪图及其特征矩阵，而鉴别器的目的是将伪样本与经验数据区分开。另外，与鉴别器并行引入奖励网络，以根据外部评估者鼓励生成的图具有某些属性。NetGAN\cite{bojchevski2018netgan}将LSTM与Wasserstein GAN \cite{arjovsky2017wasserstein}结合起来，通过基于随机游走的方法生成图形。NetGAN训练生成器通过LSTM网络生成合理的随机游走，并强制执行判别器以从真实的随机游走中识别出虚假的随机游走。训练后，通过归一化基于生成器产生的随机游走而计算出的节点的共现矩阵，可以得出新图。

虽然顺序方法和全局方法从两个不同的角度解决了图生成问题，但它们都遭受了大规模问题。一方面，顺序方法将图形线性化为序列。如果图形很大，则结果序列将相应地过长。用RNN模拟长序列是无效的。另一方面，由于全局方法一次性生成图形，GAE的输出空间高达$O(n^2)$。

